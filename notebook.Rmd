---
title: "NBA players data"
output: html_notebook
---

Ana Escobar Llamazares UO264690

Clasificación de la variable draft round usando como algoritmo Bagged AdaBoost (AdaBag).


Leer el dataset https://www.kaggle.com/justinas/nba-players-data?select=all_seasons.csv
```{r}
dataset <- read.csv("all_seasons.csv")
```


Observar los datos del dataset y sus variables:
```{r}
head(dataset)
```
```{r}
summary(dataset)
```


Convertir la variable draft_round a tipo factor (es la variable que se va a
utilizar para la clasificación, tiene 7 valores únicos).
La variable que se utiliza para aprender no puede ser de tipo char, debe ser de
tipo factor para poder pronosticar la variable de la clase.
```{r}
dataset$draft_round <- as.factor(dataset$draft_round)
```


Compruebo que se ha modificado correctamente en el dataset la variable draft_round:
```{r}
summary(dataset)
```

Bagging es una manera muy popular y efectiva de estimación muy precisa reduciendo varianza y bias del dataset

Bagging = Bootstrapping + Aggregation

En R, los paquetes 'adabag' y 'ipred' son utilizados para desarrollar modelos basados en bagging. Se utilizará el paquete 'adabag'. Cargo también el resto de paquetes necesarios.
```{r}
library(adabag)
library(rpart)
```

Selección de una submuestra del 80% de los datos para usarlos en el entrenamiento del modelo (fijo semilla para obtener siempre los mismos resultados):
```{r}
set.seed(725)
inTrain <- createDataPartition(y=dataset$draft_round, 
                               p=0.8, 
                               list=FALSE)
training <- dataset[inTrain,]
testing <- dataset[-inTrain,]
```


Creación del modelo (fijo seimlla para obtener resultados resolutivos), aplico AdaBoost (paquete AdaBag):
```{r}
set.seed(725)
adaboost <- bagging(draft_round ~ .,
                    data=training,
                    mfinal=20)
yhat_adaboost <- predict(adaboost, newdata=training)$class
```

Para saber la importancia de cada variable:
```{r}
importanceplot(adaboost)
#adaboost$importance
```

En el objeto yhat_adaboost está la predicció y con ella vamos a formar la tabla de confusión para el modelo.
```{r}
tabla_adaboost <- table(yhat_adaboost, training$draft_round)
```

Calcular la tasa de clasificación correcta, el resultado (a partir de él se puede deducir la capacidad de generalización):
```{r}
sum(diag(tabla_adaboost)) / sum(tabla_adaboost)
```

La tasa de clasificación con AdaBoost es alta, se puede concluir que la capacidad de generalización es buena.











